{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"direction:rtl\"> یادگیری تقویت شده\n",
    " </div>\n",
    "<div style=\"direction:rtl\">\n",
    "    یادگیری تقویت شده بخشی از یادگیری ماشینی است که از روانشناسی رفتاری تاثیر گرفته و بیان میدارد که چگونه یک [ایجنت نرم افزاری](https://en.wikipedia.org/wiki/Software_agent) \n",
    "   باید[   اعمالی](https://en.wikipedia.org/wiki/Action_selection) \n",
    "را در یک محیط انجام دهد تا میزان پاداش انباشته را به حداکثر برساند.\n",
    "    در یادگیری ماشینی ، محیط معمولا بصورت  [فرایندهای تصمیم‌گیری مارکوف](https://en.wikipedia.org/wiki/Markov_decision_process)\n",
    " فرموله میشود، درست همانند همه ی الگوریتم های یادگیری تقویت شده برای این مطلب از [برنامه‌نویسی پویا](https://en.wikipedia.org/wiki/Dynamic_programming) استفاده شده است.\n",
    "\n",
    "  <div style=\"direction:rtl\">\n",
    "   <br/>\n",
    "     در برخی مسائل یادگیری ماشینی ، ما بصورت مستقیم دسترسی فوری به لیبل ها(نشان ها) نداریم. بنابر این ما به تکنیک های یادگیری نظارت شده نمیتوانیم تکیه کنیم.اگر، چنانچه چیزی وجود داشته باشد که ما بتوانیم با آن تعامل کنیم به وسیله آن مقداری فیدبک بگیریم، که حرکت قبلی ما خوب یا بد بوده، میتوانی یادگیری تقویت شده را بکار گیریم تا یادبگیریم چگونه رفتارمان را بهبود ببخشیم.\n",
    "   \n",
    " </div>\n",
    " <div style=\"direction:rtl\">\n",
    "    <br/>\n",
    "    برخلاف یادگیری نظارت شده ، در یادگیری تقویتی، جفت های ورودی و خروجی نشان دار به هیچ عنوان وجود ندارد و عمل های پر اشتباه هیچوقت بصورت مستقیم درست نمیشوند. این بسیاری از پاردایم های یادگیری همزمان را شکل میدهد که درگیر یافتن یک بالانس بین ماجراجویی(شرایطی که قبلا مشاهد و یا یادگیری نشده) و سر سپردگی(شرایطی که اَعمال با استفاده از تجربیات گذشته انجام میشود) هستند.     \n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "    <br/>\n",
    "    **بیان مسئله**\n",
    "    <br/>\n",
    "    ما از شبیه ساز [سومو](http://sumo.dlr.de/wiki/Simulation_of_Urban_MObility_-_Wiki) استفاده کردیم که تمامی نکات چهارراه مورد مطالعه در آن پیاده سازی شده . این نکات شامل موارد زیر میباشد :\n",
    "    <ul>\n",
    "<li>اجرای دقیق و مو به موی لوجستیک چهارراه</li>\n",
    "<li>اجرای دقیق ترافیک ورودی به چهارراه</li>\n",
    "<li>اضافه کردن عنصر پویای چهارراه (چراغ راهنمایی و رانندگی)</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- TODO -->\n",
       "<div id=\"15110421761413674\"><script type=\"text/JavaScript\" src=\"https://www.aparat.com/embed/4psO7?data[rnddiv]=15110421761413674&data[responsive]=yes\"></script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- TODO -->\n",
    "<div id=\"15110421761413674\"><script type=\"text/JavaScript\" src=\"https://www.aparat.com/embed/4psO7?data[rnddiv]=15110421761413674&data[responsive]=yes\"></script></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "        که بوسیله آن ما به چراغ راهنمایی یاد میدهیم که بهترین زمانبندی را برگزیند.درایور پیاده سازی شده(env.py) برای تبدیل به شمای مارکوف ، رابط بین محیط شبیه ساز سومو و ایجنت ما میباشد . بدین صورت که عمل (action) مورد نظر را گرفته، آنرا در محیط چهارراه اِعمال کرده و فید  بک را بصورت یک لیست از (state)\n",
    "    ها و یک سیگنال(مقدار) پاداش (reward) بر میگرداند.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- TODO -->\n",
       "<div id=\"15111046243326414\"><script type=\"text/JavaScript\" src=\"https://www.aparat.com/embed/hSnr1?data[rnddiv]=15111046243326414&data[responsive]=yes\"></script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- TODO -->\n",
    "<div id=\"15111046243326414\"><script type=\"text/JavaScript\" src=\"https://www.aparat.com/embed/hSnr1?data[rnddiv]=15111046243326414&data[responsive]=yes\"></script></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "   نحوه پیاده سازی ای عناصر زنجیره مارکوف به صورت زیر میباشد:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>Element</th>\n",
    "    <th>Implementation</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>State</td>\n",
    "    <td>Dictionary [Queue(all edges), Number of Vehicle(a.e.), Waiting Time(a.e.)]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Action</td>\n",
    "    <td>The index of choosen seconds for Traffic light in the list of possible seconds for example : actionsForLeftPhase = [3, 4, 5, 6, 7, 8, 9, 10]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Reward</td>\n",
    "    <td>(1 * queueTracker[edge]) ** 1.75 + (2 * waitingTracker[edge]) **1.75</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "    <br/>\n",
    "    **هدف**\n",
    "    <br/>\n",
    "    ما در این پروژه قصد داریم که با استفاد از عملگرها(actions) میزان معطلی خودروها(reward)\n",
    "    را به حداقل رسانده و به مدلی از تصمیم گیری برسیم که قابلیت جامعیت داشته باشد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "اما چگونه؟\n",
    " <br/>\n",
    "    *Deep Q-Network*\n",
    " <br/>\n",
    "    DQNs در سال 2015 به شهرت رسید \n",
    "    زمانیکه نتایج موفقی از بکارگیری آنها در یادگیری بازی های آتاری بدست آمد. در این تحقیقات، ایجنت ،عمل یادگیری را فقط از راه دیدن پیکسل های صفحه انجام میدهد.\n",
    "    ما یک شبکه عصبی را تعلیم خواهیم داد تا مقدار Q(s,a) را بفهمیم.\n",
    "    بوسیله این  مقادیر Q ما بهترین عمل را برمیگزینیم.\n",
    "    \n",
    " <br/>\n",
    "    *Policy gradient*\n",
    " <br/>\n",
    " این متد مستقیماً راهکار(مجموعه ای از اکشن ها)را تخمین میزند.\n",
    "    نتیجه یک دسته از اکشن های مرتب شده است که ما را به حداکثری کردن سیگنال پاداش هدایت میکند چنانچه یک زیر دسته را برگزینیم.در این پروژه ما بوسیله\n",
    "    [گرادیان کاهشی](https://en.wikipedia.org/wiki/Gradient_descent)  به عمل های بهینه میرسیم .\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl\">\n",
    "    در این پروژه ، تمرکز ما بر روی چگونگی پیاده سازی یادگیری تقویتی است بوسیله CNTK.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
